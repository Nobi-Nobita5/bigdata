#### 第一章 数据采集和分流

##### 一、日志数据采集和分流

1. **ODS层**数据采集直接从日志系统发送到kafka，没有用flume采集文件

2. 消费者将数据分流使用Dstream API，没有使用spark structured streaming。

3. **精确一次消费问题**：默认偏移量Offset自动提交，如果在【写出数据（数据存盘）】之前，进行崩溃，且kafka已自动提交偏移量，那么进程重启之后，会从新的偏移量开始，之前没有被保存的数据就丢失了。

   两种解决办法：

   1）将消费者【写出数据】和【提交offset】的动作，用关系型数据库的事务进行处理

   2）在【写出数据】之后，在Driver端手动提交偏移量，【一个数据**批次**foreachRDD】提交一次，保证数据不会丢失。

   ​      幂等性处理，保证数据不重复消费。（看使用的数据库，一般有主键的数据库都支持幂等性操作，实现幂等操作的方法有很多，如 upsert，即 UPDATE + INSERT。）。

   ​      注：项目中此处kafka 中的数据只是用于中间存储,并不会进行统计，所以只要保证不丢失即可，重复数据的幂等性处理可以交给下游处理。故使用方法2。

4. **生产者发送数据到kafka的缓冲区问题**：生产者会先将消息发送到kafka的缓冲区中，如果缓冲区的数据还没有刷写到Broker，此时kafka集群故障，且offset已提交，此部分的数据就会丢失。

   两种解决办法：

   1）将消息改为同步发送，foreach()中每缓存一条数据就flush()发送到【kafka集群(磁盘)】中。【会牺牲性能。故用下面的方法。】

   2）在手动提交 offset 之前，在【executor端（每个分区）】强制将缓冲区的数据 【flush 到 broker】，具体操作：

   ​	  使用rdd.foreachPartition()算子替换掉foreach()算子。foreach()算子里面的代码会在【每个executor端】执行，rdd.foreachPartition()算子里的代码也是在【每个executor端】执行，但是只会【每批次每分区执行一次】。详见com.atguigu.gmall.realtime.app.OdsBaseLogApp.scala代码。

   ​	该操作也只能保证数据不会丢失，不能保证数据不重复消费。
   
   注：项目中此处的生产者是指代码的中间生产者，负责将ODS层的中间消费者处理的数据发送到DWD层的kafka主题，【故才需要考虑生产者 flush 的时机】，如果DWD层不是发送到kafka，可能不会存在缓冲区问题。

##### 二、业务数据采集和分流

1. 基本操作与第一节相同，但实时的【业务数据】来源自Maxwell对MySQL数据库的监控，Maxwell监控到变化数据自动发送至kafka的某个Topic，然后spark程序再消费kafka数据分流到DWD层。

2. 【业务数据】分为【事实表和维度表】，将【事实表】分流到DWD层不同的topic中，将【维度表】的数据写入到redis中。这里有两个需要注意的问题：

   1）用于判断业务数据种类的事实表和维度表的动态表清单，配置在redis中，方便动态修改，不影响程序运行。

   2）业务数据的分流涉及到两处地方使用了redis：

   ​	  一处redis连接创建在【foreachRDD里面, foreachPartition外面】，方便每批次使用该链接动态读取配置表，然后做成广播变量发送到各个executor。

   ​	  另一处redis链接创建在【foreachRDD里面, foreachPartition里面】，方便每批次每分区存放维度表数据。由于连接对象不能序列化，不能传输，故获取连接的操作具体是在【foreachPartition里面 , jsonObjIter迭代器循环外面】执行。使得每分区数据开启一个连接，用完关闭。

   3）历史的维度表数据，也许没有通过maxwell监控Mysql从而将数据自动发送至kafka的某个Topic中，所以我们需要使用maxwell的`./maxwell-bootstrap --config ../config.properties --database gmall --table user_info`功能将MySQL中的历史数据全量同步到kafka中。

3. 第一节的两个问题，解决办法依然是：

   1）每个批次处理的最后，在Driver端提交偏移量Offset（不能保证数据不重复消费）

   2）在Driver端手动提交 offset 之前，在【每批次的每个分区（executor端）】强制将缓冲区的数据 【flush 到 broker】

4. **kafka顺序消费问题**：同一条数据在MySQL中连续修改A,B,C，Maxwell监控到改动发送给kafka，【由于kafka存在多个分区，一条数据的改动可能发往不同的分区】。Spark在消费kafka的数据时，从不同的分区中拿到同一条数据的3次改动，可能是乱序的，也就是说最终结果可能不是C。

   解决办法：修改Maxwell配置文件，将同一条数据的修改发送到同一个分区中。

   ~~~shell
   #分区的方式
   producer_partition_by=column
   #分区的字段，保证该字段唯一标识的一条数据在一个分区
   producer_partition_columns=id
   #字段不存在，用表分区，保证一个表的修改在一个分区
   producer_partition_by_fallback=table
   ~~~

   注：

   spark如果拿到有序的数据，由于spark可能使用的一些【涉及重分区的算子】，也不能保证数据的有序性。

   spark如果拿到无序的数据，也可以考虑【再分区将数据根据操作时间进行排序】，从而达到有序。

   另外，与需要手动编码再分区的Dstream不同，【spark structured streaming API】能很轻松解决【乱序】和【迟到】数据的问题，它可以根据【事件时间】处理流数据。
   
   #### 第二章 分层处理
   
   ##### 一、DWD 到 DWS 层数据处理概要
   
   1. ODS到DWD层主要负责原始数据的整理拆分，形成一个个的业务事实topic
   2. DWD到DWS层主要负责把单个业务事实topic变为【面向统计的事实明细宽表】，然后保存到OLAP中（本项目中OLAP引擎使用的ES。ES作为本项目数据仓库模型的DWS层）。
   3. 我们利用OLAP的优点进行数据分析。而OLAP 数据库不擅长的 join 、复杂数据计算、去重等操作，就在实时计算的spark程序中完成。
   
   ##### 二、日活宽表
   
   1. 去重
   
      日活数据来源自日志数据，由于用户可能在一天之内多次访问系统，另外我们在第一章对【精确一次消费问题】的处理也没有保证数据不重复消费，所以对于日活统计来说DWD层会有很多重复数据，需要在DWD层日活数据传入DWS层之前，做去重操作。
   
      1）【自我审查】：将页面访问数据中last_page_id不为空的数据**过滤**掉（同一次访问系统，对访问不同页面的访问数据进行去重）。
   
      2）【第三方审查】：通过redis将当日活跃的mid维护起来,自我审查后的每条数据需要到redis中进行比对去重（对非同一时间登录系统的访问数据进行**过滤**）。
   
   2. 维度关联
   
      ​		维度数据已经保存在Redis中，所以在实时计算时，维度数据和事实数据关联不需要通过join算子完成。而是在流中查询固定容器Redis来实现维度补充。
   
      ​		注：由于要针对不同角度的‘日活’分析，而OLAP数据库中要【尽量减少】join操作，所以在spark实时计算程序中（即日活数据传入DWS层之前）就要【充分考虑】其会被分析的维度，【补充响应相应的维度数据，形成宽表】。
   
   3. 输出到OLAP
   
      1）批量写入：减少IO次数，提高写入性能
   
      2）幂等性写入：保证数据的精确一次消费
   
   4. 【状态数据还原问题】：
   
      ​		想象一个比较极端的情况， 如果某个用户某天的首次访问数据写入 redis 后， 接下来在写入到 es 的过程中,程序挂掉。 会出现什么问题 ?
   
      ​		程序挂掉，偏移量还未提交，重启后会触发数据的重试，但是因为【第三方审查】 时redis 中记录了相关的数据，所以该数据会被过滤掉。因此此数据，就再也无法进入 es，也就意味着丢失。这个问题的本质就是，状态数据与最终数据库的数据以及偏移量，没有形成原子性事务造成的。当然可以通过事务数据库的方式解决该问题，而我们的项目中没有选择使用支持事务的数据库，例如 MySQL 等。在既有的环境下我们依然有很多破解方案，例如【进行状态还原，在启动程序前，以ES中的数据为准，将 ES 中已有的数据的 mid 提取出来，覆盖到 Redis 中，这样就能保证Redis 和 ES 数据的同步】。
   
   ##### 三、订单业务宽表
   
   1. 流与维度关联：
   
      ​		维度数据保存在Redis中，处理流数据时，读取缓存关联维度数据。
   
   2. 双流Join：
   
      ```
      // 解决: 最终方案：【每个批次没有根据id关联上的表的数据，存入缓存】。
      //  1. 扩大采集周期 ，一个大周期执行一个微型批， 确保能关联。这样还搞啥实时，搞离线算球了
      //  2. 使用窗口,治标不治本 , 还要考虑数据去重（这里是双流JOIN，与单流式聚合不同，会导致窗口内的数据不断关联产生重复数据） 、 Spark状态的缺点（窗口越大，需要的资源越多）
      //  3. 首先使用fullOuterJoin,保证join成功或者没有成功的数据都出现到结果中.
      //     让双方都多两步操作, 到缓存中找对的人， 把自己写到缓存中。structure streaming的双流JOIN也是采用的第三种策略。
      //     1）structure streaming中这种策略 会设置水印和事件时间的约束条件，spark引擎会自动计算状态保留时间，不然状态会无限增长，内存会不够用。
      //     2）此处我们手动设置了redis过期时间为24小时。严格来说也需要实现，设置水印和事件时间的约束所达到的效果。不然24小时，数据量大的话很可能内存就不够了。
      ```
