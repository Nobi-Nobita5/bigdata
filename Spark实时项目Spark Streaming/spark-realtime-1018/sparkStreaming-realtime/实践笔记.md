#### 第一章 数据采集和分流

##### 一、日志数据采集和分流

1. ODS层数据采集直接从日志系统发送到kafka，没有用flume采集文件

2. 消费者将数据分流使用Dstream API，没有使用spark structured streaming。

3. 问题，偏移量Offset自动提交不能实现消费者【精确一次消费】的两种解决办法：

   1）将消费者【写出数据】和【提交offset】的动作，用关系型数据库的事务进行处理

   2）在【写出数据】之后，在Driver端手动提交偏移量，【一个数据**批次**foreachRDD】提交一次，保证数据不会丢失。

   ​      幂等性处理，保证数据不重复消费。（看使用的数据库，一般有主键的数据库都支持幂等性操作 upsert）。

   ​      注：项目中此处kafka 中的数据只是用于中间存储,并不会进行统计，所以只要保证不丢失即可，重复数据的幂等性处理可以交给下游处理。故使用方法2。

4. 问题，【生产者发送数据到kafka的缓冲区问题】。生产者会先将消息发送到缓冲区中，如果缓冲区的数据还没有刷写到Broker，此时kafka集群故障，且offset已提交，此部分的数据就会丢失。两种解决办法：

   1）将消息改为同步发送，foreach()中每缓存一条数据就flush()发送到【kafka集群(磁盘)】中。【会牺牲性能。故用下面的方法。】

   2）在手动提交 offset 之前，在【executor端（每个分区）】强制将缓冲区的数据 【flush 到 broker】，具体操作：

   ​	  使用rdd.foreachPartition()算子替换掉foreach()算子。foreach()算子里面的代码会在【每个executor端】执行，rdd.foreachPartition()算子里的代码也是在【每个executor端】执行，但是只会【每批次每分区执行一次】。详见com.atguigu.gmall.realtime.app.OdsBaseLogApp.scala代码。

   ​	该操作也只能保证数据不会丢失，不能保证数据不重复消费。

   注：项目中此处的生产者是指代码的中间生产者，负责将ODS层的中间消费者处理的数据发送到DWD，【故需要考虑消费者提交offset的时机和 flush 的时机】。

##### 二、业务数据采集和分流

1. 基本操作与第一节相同，但实时的业务数据来源自Maxwell对MySQL数据库的监控，Maxwell监控到变化数据自动发送至kafka的一个Topic，然后spark程序再消费kafka数据进行分流处理。

2. 业务数据分为【事实表和维度表】，将【事实表】分流到不同的topic中，将【维度表】的数据写入到redis中。这里有两个需要注意的问题：

   1）事实表和维度表的动态表清单，配置在redis中，方便动态修改，不影响程序运行。

   ​	  一处redis连接写在【foreachRDD里面, foreachPartition外面】，方便每批次动态读取配置表。

   ​	  另一处redis链接写在【foreachRDD里面, foreachPartition里面】，方便每批次每分区存放维度表数据。

   2）维度表的历史数据，使用maxwell的`./maxwell-bootstrap --config ../config.properties --database gmall --table user_info`功能将MySQL中的历史数据全量同步到kafka中。

   3）用于存放维度数据的redis，由于连接对象不能序列化，不能传输，故获取连接的操作在【foreachPartition里面 , 循环外面】执行。使得每分区数据开启一个连接，用完关闭。

3. 第一节的两个问题，解决办法依然是：

   1）每个批次处理的最后，在Driver端提交偏移量Offset（不能保证数据不重复消费）

   2）在Driver端手动提交 offset 之前，在【executor端（每个分区）】强制将缓冲区的数据 【flush 到 broker】

4. 【数据处理顺序性问题】：同一条数据在MySQL中连续修改A,B,C，Maxwell监控到改动发送给kafka，【由于kafka存在多个分区，一条数据的改动可能发往不同的分区】。Spark在消费kafka的数据时，从不同的分区中拿到同一条数据的3次改动，可能是乱序的，也就是说最终结果可能不是C。

   解决办法：修改Maxwell配置文件，将同一条数据的修改发送到同一个分区中。

   ~~~shell
   #分区的方式
   producer_partition_by=column
   #分区的字段，保证该字段唯一标识的一条数据在一个分区
   producer_partition_columns=id
   #字段不存在，用表分区，保证一个表的修改在一个分区
   producer_partition_by_fallback=table
   ~~~

   注：

   spark如果拿到有序的数据，由于spark可能使用的一些【涉及重分区的算子】，也不能保证数据的有序性。

   spark如果拿到无序的数据，也可以考虑【再分区将数据根据操作时间进行排序】，从而达到有序。

   另外，与需要手动编码再分区的Dstream不同，【spark structured streaming API】能很轻松解决【乱序】和【迟到】数据的问题，它可以根据【事件时间】处理流数据。
   
   #### 第二章 分层处理
   
   ##### 一、DWD 到 DWS 层数据处理概要
   
   1. ODS到DWD层主要负责原始数据的整理拆分，形成一个个的业务事实topic
   2. DWD到DWS层主要负责把单个业务事实topic变为【面向统计的事实明细宽表】，然后保存到OLAP中（本项目中OLAP引擎使用的ES）。
   3. 我们利用OLAP的优点进行数据分析。而OLAP 数据库不擅长的 join 、复杂数据计算、去重等操作，就在实时计算的spark程序中完成。
   
   ##### 二、日活宽表
   
   1. 去重
   
      日活数据来源自日志数据，有很多重复数据，需要在将日活数据传入ES之前，做去重操作。
   
      1）【自我审查】：将页面访问数据中last_page_id不为空的数据过滤掉（过滤掉当日非第一次访问系统的数据）。
   
      2）【第三方审查】：通过redis将当日活跃的mid维护起来,自我审查后的每条数据需要到redis中进行比对去重（对非同一时间登录系统的访问数据进行去重）。
   
   2. 维度关联
   
      ​		由于要针对不同角度的‘日活’分析，而OLAP数据库中要【尽量减少】join操作，所以在spark实时计算程序中要考虑其会被分析的维度，【补充响应相应的维度数据，形成宽表】。
   
      ​		由于维度数据已经保存在Redis中，所以在实时计算时，维度数据和事实数据关联并不是通过join算子完成。而是在流中查询固定容器Redis来实现维度补充。
   
   3. 输出到OLAP
   
      1）批量写入：减少IO次数，提高写入性能
   
      2）幂等性写入：保证数据的精确一次消费
   
   4. 【状态数据还原问题】：
   
      ​		想象一个比较极端的情况， 如果某个用户某天的首次访问数据写入 redis 后， 接下来在写入到 es 的过程中,程序挂掉。 会出现什么问题 ?
   
      ​		程序挂掉，偏移量还未提交，重启后会触发数据的重试，但是因为 redis 中记录了相关的数据，所以该数据会被过滤掉。因此此数据，就再也无法进入 es，也就意味着丢失。这个问题的本质就是，状态数据与最终数据库的数据以及偏移量，没有形成原子性事务造成的。当然可以通过事务数据库的方式解决该问题，而我们的项目中没有选择使用支持事务的数据库，例如 MySQL 等。在既有的环境下我们依然有很多破解方案，例如【进行状态还原，在启动程序前，将 ES 中已有的数据的 mid 提取出来，覆盖到 Redis 中，这样就能保证Redis 和 ES 数据的同步】。
   
   ##### 三、订单业务宽表
   
   
