##### 针对面试总结

   1. 功能总结：

      > 1. ```
      >    * 【日志页面访问事实数据流】关联【用户维度数据】
      >      * 用户日活宽表，用于：
      >      *  1.统计每日活跃用户
      >      *  2.分析用户地域和网站访问量的关系，用户年龄和网站访问量的关系
      >    ```
      >
      > 2. ```
      >    * 【日志页面访问事实数据流】 和 【博客信息事实数据流】关联的宽表，实现：
      >    *  1.实时推荐文章，通过该宽表得到的用户最近访问、搜索过的文章类别，或与该用户地区、年龄信息相近的用户访问、搜索过的文章类别，推荐相同类型文章在主页显示。
      >    *  2.分析用户年龄、性别和博客类别的关系，得出结论：什么年龄喜欢什么类型的博客
      >    *  3.实时热点分析：分析热点分类、热门标签等。
      >    ```
      >
      > 
      >
      > 3. ```
      >    * 【页面访问事实数据】关联【用户维度数据】，进行自我审查，不进行第三方审查（即不过滤一天内不同时间段的访问）
      >      * 用户访问宽表，用于：
      >      *  1.监控网站的访问量PV、UV、响应时间等指标，及时发现并处理流量问题，确保网站的访问速度和稳定性。
      >    ```

2. 数据来源总结：

   > 1. 日志数据在博客系统使用AOP实现的日志处理得到，直接发送到kafka
   > 2. 事实数据通过maxwell监控mysql数据库变动记录直接发送到kafka

3. 遇到的问题总结：

   > 1. Redis的使用：
   >
   >    1）使用Redis手动提交kafka消费偏移量（一个批次提交一次），解决**Kafka的精确一次消费问题**。
   >
   >    ```
   >    *          类型: hash
   >    *          key : topic + groupId
   >    *          value: partition - offset  ， partition - offset 。。。。
   >    *          写入API: hset / hmset
   >    *          读取API: hgetall
   >    *          是否过期: 不过期
   >    ```
   >
   >    2）使用Redis存放当日访问过网站的设备mid，为日活统计解决**用户行为数据去重问题**。
   >
   >    ```
   >    // 类型:    set
   >    // key :    DAU:DATE
   >    // value :  mid的集合
   >    // 写入API: sadd
   >    // 读取API: smembers
   >    // 过期:  24小时
   >    ```
   >
   >    3）使用Redis存放维度表表名，事实表表名，用于动态配置表清单，将维度表存放在Redis
   >
   >    ```
   >    //如何动态配置表清单???
   >    // 将表清单维护到redis中，实时任务中动态的到redis中获取表清单.
   >    // 类型: set
   >    // key:  FACT:TABLES   DIM:TABLES
   >    // value : 表名的集合
   >    // 写入API: sadd
   >    // 读取API: smembers
   >    // 过期: 不过期
   >    ```
   >
   >    4）使用Redis存放维度数据，用于补充DWD宽表
   >
   >    ```
   >    // 类型 : list，set，zset : key为表名，多个value为一条数据，不可行，因为不方便定位每一条数据。
   >    //                          key为主键id，多个value为每个字段的数据，不可行，因为不方便定位每个字段的数据。
   >    //                          故集合与列表都不合适。
   >    //        hash ： 整个表存成一个hash。key是表名，value是主键id和一条数据。 要考虑目前单表数据量大小和将来数据量增长问题 及 高频访问问题。要知道一个key是存放在redis集群中的一个节点上的。
   >    //        hash :  一条数据存成一个hash。 key是主键id，value是字段名和数据。没有单独调用某个字段的场景，整查一条数据需要解析很多个field(字段)。
   >    //        String : 一条数据存成一个jsonString. key是主键id，value是json字符串。
   >    //                 不用担心单表数据量过大，可以让redis集群负载均衡；整查一条数据也比hash方便。
   >    //                 故最终采用String方案。
   >    // key :  DIM:表名:主键ID
   >    // value : 整条数据的jsonString
   >    // 写入API: set
   >    // 读取API: get
   >    // 过期:  不过期（维度数据）
   >    ```
   >
   >    5）双流Join解决方案，使用Redis存放双流全关联时，未匹配到的数据。
   >
   >    ```
   >    // 解决: 最终方案：【每个批次没有根据id关联上的表的数据，存入缓存】。
   >    //  1. 扩大采集周期 ，一个大周期执行一个微型批， 确保能关联。这样还搞啥实时，搞离线算球了
   >    //  2. 使用窗口,治标不治本 , 还要考虑数据去重（这里是双流JOIN，与单流式聚合不同，会导致窗口内的数据不断关联产生重复数据） 、 Spark状态的缺点（窗口越大，需要的资源越多）
   >    //  3. 首先使用fullOuterJoin,保证join成功或者没有成功的数据都出现到结果中.
   >    //     让双方都多两步操作, 到缓存中找对的人， 把自己写到缓存中。structure streaming的双流JOIN也是采用的第三种策略。
   >    //     1）structure streaming中这种策略 会设置水印和事件时间的约束条件，spark引擎会自动计算状态保留时间，不然状态会无限增长，内存会不够用。
   >    //     2）此处我们手动设置了redis过期时间为2小时。严格来说也需要实现，设置水印和事件时间的约束所达到的效果。不然2小时，数据量大的话很可能内存就不够了。
   >    ```
   >
   >    ```
   >    //orderInfo写缓存
   >    // 类型:  string
   >    // key :   ORDERJOIN:ORDER_INFO:ID
   >    // value :  json
   >    // 写入API:  set
   >    // 读取API:  get
   >    // 是否过期: 24小时
   >    ```
   >
   > 1. **解决状态还原问题**
   >
   >    > 想象一个比较极端的情况， 如果某个用户某天的首次访问数据写入 redis 后， 接下来在写入到 es 的过程中,程序挂掉。 会出现什么问题 ?
   >    >
   >    > ​		程序挂掉，虽然偏移量还未提交，重启后会触发数据的重试，但是因为【第三方审查】 时redis 中记录了相关的数据，所以该数据会被过滤掉。因此此数据，就再也无法进入 es，也就意味着丢失。这个问题的本质就是，状态数据与最终数据库的数据以及偏移量，没有形成原子性事务造成的。当然可以通过事务数据库的方式解决该问题，而我们的项目中没有选择使用支持事务的数据库，例如 MySQL 等。在既有的环境下我们依然有很多破解方案，例如【进行状态还原，在启动程序前，以ES中的数据为准，将 ES 中已有的数据的 mid 提取出来，覆盖到 Redis 中，这样就能保证Redis 和 ES 数据的同步】。
   >
   > 1. **解决kafka顺序消费问题**：
   >
   >    同一条数据在MySQL中连续修改A,B,C，Maxwell监控到改动发送给kafka，【由于kafka存在多个分区，一条数据的改动可能发往不同的分区】。Spark在消费kafka的数据时，从不同的分区中拿到同一条数据的3次改动，可能是乱序的，也就是说最终结果可能不是C。
   >
   >    解决办法：修改Maxwell配置文件，将同一条数据的修改发送到同一个分区中。
   >
   >    ~~~shell
   >    #分区的方式
   >    producer_partition_by=column
   >    #分区的字段，保证该字段唯一标识的一条数据在一个分区
   >    producer_partition_columns=id
   >    #字段不存在，用表分区，保证一个表的修改在一个分区
   >    producer_partition_by_fallback=table
   >    ~~~
   >
   >    注：
   >
   >    spark如果拿到有序的数据，由于spark可能使用的一些【涉及重分区的算子】，也不能保证数据的有序性。
   >
   >    spark如果拿到无序的数据，也可以考虑【再分区将数据根据操作时间进行排序】，从而达到有序。
   >
   >    另外，与需要手动编码再分区的Dstream不同，【spark structured streaming API】能很轻松解决【乱序】和【迟到】数据的问题，它可以根据【事件时间】处理流数据。
   >
   > 1. 解决**生产者发送数据到kafka的缓冲区问题**（每批次每分区提交一次）：
   >
   >    生产者会先将消息发送到kafka的缓冲区中，如果缓冲区的数据还没有刷写到Broker，此时kafka集群故障，且offset已提交，此部分的数据就会丢失。
   >
   >    两种解决办法：
   >
   >    1）将消息改为同步发送，foreach()中每缓存一条数据就flush()发送到【kafka集群(磁盘)】中。【会牺牲性能。故用下面的方法。】
   >
   >    2）在手动提交 offset 之前，在【executor端（每个分区）】强制将缓冲区的数据 【flush 到 broker】，具体操作：
   >
   >    ​	  使用rdd.foreachPartition()算子替换掉foreach()算子。foreach()算子里面的代码会在【每个executor端】执行，rdd.foreachPartition()算子里的代码也是在【每个executor端】执行，但是只会【每批次每分区执行一次】。详见com.atguigu.gmall.realtime.app.OdsBaseLogApp.scala代码。
   >
   >    ​	该操作也只能保证数据不会丢失，不能保证数据不重复消费。
   >
   >    注：项目中此处的生产者是指代码的中间生产者，负责将ODS层的中间消费者处理的数据发送到DWD层的kafka主题，【故才需要考虑生产者 flush 的时机】，如果DWD层不是发送到kafka，可能不会存在缓冲区问题。